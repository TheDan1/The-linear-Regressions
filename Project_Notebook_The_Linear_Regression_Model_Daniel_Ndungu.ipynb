{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project Notebook: The Linear Regression Model-Daniel Ndungu",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9znpyK4UbceY"
      },
      "source": [
        "# Project Notebook: The Linear Regression Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JtBkHv4bdbL"
      },
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q7QTPAhbjGj"
      },
      "source": [
        "\n",
        "We started by building intuition for model based learning, explored how the linear regression model worked, understood how the two different approaches to model fitting worked, and some techniques for cleaning, transforming, and selecting features. In this project, you can practice what you learned by exploring ways to improve the models we built.\n",
        "\n",
        "You'll work with housing data for the city of Ames, Iowa, United States from 2006 to 2010. You can also read about the different columns in the data [here](https://s3.amazonaws.com/dq-content/307/data_description.txt).\n",
        "\n",
        "Let's start by setting up a pipeline of functions that will let us quickly iterate on different models.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. Import pandas, matplotlib, and numpy into the environment. Import the classes you need from scikit-learn as well.\n",
        "2. Read `AmesHousing.tsv` () into a pandas data frame.\n",
        "3. For the following functions, we recommend creating them in the first few cells in the notebook. This way, you can add cells to the end of the notebook to do experiments and update the functions in these cells.\n",
        "* Create a function named `transform_features()` that, for now, just returns the train data frame.\n",
        "* Create a function named `select_features()` that, for now, just returns the Gr Liv Area and SalePrice columns from the train data frame.\n",
        "* Create a function named `train_and_test()` that, for now:\n",
        "\n",
        "1. Selects the first 1460 rows from from data and assign to train.\n",
        "2. Selects the remaining rows from data and assign to test.\n",
        "3. Trains a model using all numerical columns except the SalePrice column (the target column) from the data frame returned from `select_features()`\n",
        "4. Tests the model on the test set and returns the `RMSE` value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTUjKMIObXHq"
      },
      "source": [
        "# Your code goes here\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matpotlib as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# create  function to transform features\n",
        "def transform_features(df):\n",
        "    return df\n",
        " # create  function to select features\n",
        "\n",
        "def select_features(df):\n",
        "      return df[['Gr Liv Area','SalePrice']]\n",
        "\n",
        "# create function to train and test the model\n",
        "def train_and_test(df):\n",
        "    train = df[:1460]\n",
        "    test = df[1460:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUyr-Mz0c-DG"
      },
      "source": [
        "## 2. Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfxdsJbvc_jo"
      },
      "source": [
        "Let's now start removing features with many missing values, diving deeper into potential categorical features, and transforming text and numerical columns. Update `transform_features()` so that any column from the data frame with more than 25% (or another cutoff value) missing values is dropped. You also need to remove any columns that leak information about the sale (e.g. like the year the sale happened). In general, the goal of this function is to:\n",
        "\n",
        "* remove features that we don't want to use in the model, just based on the number of missing values or data leakage.\n",
        "* transform features into the proper format (numerical to categorical, scaling numerical, filling in missing values, etc).\n",
        "* create new features by combining other features.\n",
        "\n",
        "Next, you need to get more familiar with the remaining columns by reading the data documentation for each column, determining what transformations are necessary (if any), and more. As we mentioned earlier, succeeding in predictive modeling (and competitions like Kaggle) is highly dependent on the quality of features the model has. Libraries like scikit-learn have made it quick and easy to simply try and tweak many different models, but cleaning, selecting, and transforming features are still more of an art that requires a bit of human ingenuity.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. As we mentioned earlier, we recommend adding some cells to explore and experiment with different features (before rewriting these functions).\n",
        "\n",
        "2. The `transform_features()` function shouldn't modify the train data frame and instead return a new one entirely. This way, we can keep using train in the experimentation cells.\n",
        "\n",
        "3. Which columns contain less than 5% missing values?\n",
        "* For numerical columns that meet this criteria, let's fill in the missing values using the most popular value for that column.\n",
        "\n",
        "4. What new features can we create, that better capture the information in some of the features?\n",
        "* An example of this would be the `years_until_remod` feature we created in the last lesson.\n",
        "\n",
        "5. Which columns need to be dropped for other reasons?\n",
        "* Which columns aren't useful for machine learning?\n",
        "* Which columns leak data about the final sale?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctVTQR07dfJE"
      },
      "source": [
        "# drop column with 5% or more missing values\n",
        "missing = data.isnull().sum()\n",
        "missing_col = (missing[missing > len(data)/20]).index\n",
        "data = data.drop(missing_col, axis=1)\n",
        "\n",
        "# drop text columns with 1 or more missing values\n",
        "text_col = data.select_dtypes(include=['object']).columns\n",
        "missing_text = data[text_col].isnull().sum()\n",
        "ms_text_col = missing_text[missing_text >= 1].index\n",
        "data = data.drop(ms_text_col, axis=1)\n",
        "\n",
        "#fill missing value in numerical column with the most frequent value\n",
        "num_col = data.select_dtypes(include=['integer','float']).columns\n",
        "num_miss = data[num_col].isnull().sum()\n",
        "num_miss_cols = num_miss[num_miss > 0].index\n",
        "data[num_miss_cols] = data[num_miss_cols].fillna(data[num_miss_cols].mode().iloc[0])\n",
        "data.isnull().sum().sort_values()\n",
        "\n",
        "\n",
        "# create new feature to indicate the amount of years until the house is sold\n",
        "data['years_sold'] = data['Yr Sold'] - data['Year Built']\n",
        "\n",
        "# check for incorrect value\n",
        "data['years_sold'][data['years_sold'] < 0]\n",
        "\n",
        "# create new feature to indicate the amount of years until it's being renovated from sale time\n",
        "data['years_since_remod'] = data['Yr Sold'] - data['Year Remod/Add']\n",
        "\n",
        "# check for incorrect value\n",
        "data['years_since_remod'][data['years_since_remod'] < 0]\n",
        "\n",
        "\n",
        "# drop the incorrect values from previous step\n",
        "data = data.drop([2180, 1702, 2180, 2181], axis=0)\n",
        "\n",
        "# remove the original column (not needed anymore)\n",
        "data = data.drop(['Yr Sold', 'Year Remod/Add', 'Year Built'], axis=1)\n",
        "\n",
        "\n",
        "# drop the columns that are not useful for the model\n",
        "data = data.drop(['PID', 'Order'], axis=1)\n",
        "\n",
        "# drop the columns that leak information about the sale\n",
        "data = data.drop(['Mo Sold', 'Sale Type', 'Sale Condition'], axis=1)\n",
        "\n",
        "def transform_features(df):\n",
        "    # drop column with 5% or more missing values\n",
        "    missing = df.isnull().sum()\n",
        "    missing_col = (missing[missing > len(df)/20]).index\n",
        "    df = df.drop(missing_col, axis=1)\n",
        "    \n",
        "    # drop text columns with 1 or more missing values\n",
        "    text_col = df.select_dtypes(include=['object']).columns\n",
        "    missing_text = df[text_col].isnull().sum()\n",
        "    ms_text_col = missing_text[missing_text >= 1].index\n",
        "    df = df.drop(ms_text_col, axis=1)\n",
        "    \n",
        "    #fill missing value in numerical column with the most frequent value\n",
        "    num_col = df.select_dtypes(include=['integer','float']).columns\n",
        "    num_miss = df[num_col].isnull().sum()\n",
        "    num_miss_cols = num_miss[num_miss > 0].index\n",
        "    df[num_miss_cols] = df[num_miss_cols].fillna(df[num_miss_cols].mode().iloc[0])\n",
        "    \n",
        "    # create new features\n",
        "    df['years_sold'] = df['Yr Sold'] - df['Year Built']\n",
        "    df['years_since_remod'] = df['Yr Sold'] - df['Year Remod/Add']\n",
        "    \n",
        "    df = df.drop([2180, 1702, 2180, 2181], axis=0)\n",
        "\n",
        "    # drop not needed & leaking columns\n",
        "    df = df.drop(['Yr Sold', 'Year Remod/Add', 'Year Built', 'PID', 'Order', 'Mo Sold',\n",
        "                  'Sale Type', 'Sale Condition'], axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "    # test the function\n",
        "df = pd.read_csv('/kaggle/input/ameshousingdataset/AmesHousing.tsv', delimiter='\\t')\n",
        "transformed = transform_features(df)\n",
        "selected_features = select_features(transformed)\n",
        "test = train_and_test(selected_features)\n",
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4fXZPVfd4IY"
      },
      "source": [
        "## 3. Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnGMP3LYd8fZ"
      },
      "source": [
        "Now that we have cleaned and transformed a lot of the features in the data set, it's time to move on to feature selection for numerical features.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. Generate a correlation heatmap matrix of the numerical features in the training data set.\n",
        "* Which features correlate strongly with our target column, `SalePrice`?\n",
        "* Calculate the correlation coefficients for the columns that seem to correlate well with `SalePrice`. Because we have a pipeline in place, it's easy to try different features and see which features result in a better cross validation score.\n",
        "\n",
        "2. Which columns in the data frame should be converted to the categorical data type? All of the columns that can be categorized as nominal variables are candidates for being converted to categorical. Here are some other things you should think about:\n",
        "* If a categorical column has hundreds of unique values (or categories), should you keep it? When you dummy code this column, hundreds of columns will need to be added back to the data frame.\n",
        "* Which categorical columns have a few unique values but more than 95% of the values in the column belong to a specific category? This would be similar to a low variance numerical feature (no variability in the data for the model to capture).\n",
        "\n",
        "3. Which columns are currently numerical but need to be encoded as categorical instead (because the numbers don't have any semantic meaning)?\n",
        "\n",
        "4. What are some ways we can explore which categorical columns \"correlate\" well with `SalePrice`?\n",
        "\n",
        "5. Update the logic for the `select_features()` function. This function should take in the new, modified train and test data frames that were returned from `transform_features()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcR5-9-cemHN"
      },
      "source": [
        "# Your code goes here\n",
        "# check the numerical columns\n",
        "num_df = transformed.select_dtypes(include = ['float', 'integer'])\n",
        "num_df\n",
        "\n",
        "# build the correlation with target column\n",
        "corr = num_df.corr()['SalePrice'].abs().sort_values()\n",
        "corr\n",
        "\n",
        "# drop the low correlating columns\n",
        "transformed = transformed.drop(corr[corr < 0.4].index, axis=1)\n",
        "\n",
        "# Create a list of column names from documentation that should be categorical\n",
        "nominal_features = [\"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \"Land Contour\", \"Lot Config\", \"Neighborhood\", \n",
        "                    \"Condition 1\", \"Condition 2\", \"Bldg Type\", \"House Style\", \"Roof Style\", \"Roof Matl\", \"Exterior 1st\", \n",
        "                    \"Exterior 2nd\", \"Mas Vnr Type\", \"Foundation\", \"Heating\", \"Central Air\", \"Garage Type\", \n",
        "                    \"Misc Feature\", \"Sale Type\", \"Sale Condition\"]\n",
        "\n",
        " # list the columns that need to be transformed\n",
        "trans_col = []\n",
        "for i in nominal_features:\n",
        "    if i in transformed.columns:\n",
        "        trans_col.append(i)\n",
        "\n",
        "# find out the unique values in each column\n",
        "unique_counts = transformed[trans_col].apply(lambda x: len(x.value_counts())).sort_values()\n",
        "\n",
        "# set the threshold for the amount of unique values. Here we will use column with <10 unique values.\n",
        "nonunique_counts = unique_counts[unique_counts > 10]\n",
        "\n",
        "# drop the columns with unique values >10\n",
        "transformed = transformed.drop(nonunique_counts.index, axis=1) \n",
        "\n",
        "#Update the logic for the select_features() function. This function should take in the new, modified train and test data frames that were returned from transform_features()\n",
        "\n",
        "def select_features(df, corrval=0.4, threshval=10):\n",
        "    # check the numerical columns\n",
        "    num_df = df.select_dtypes(include = ['float', 'int'])\n",
        "    \n",
        "    # build the correlation with target column\n",
        "    corr = num_df.corr()['SalePrice'].abs().sort_values()\n",
        "    \n",
        "    # drop the low correlating columns\n",
        "    df = df.drop(corr[corr < corrval].index, axis=1)\n",
        "    \n",
        "    # List the categorical columns\n",
        "    nominal_features = [\"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \"Land Contour\", \"Lot Config\", \"Neighborhood\", \n",
        "                    \"Condition 1\", \"Condition 2\", \"Bldg Type\", \"House Style\", \"Roof Style\", \"Roof Matl\", \"Exterior 1st\", \n",
        "                    \"Exterior 2nd\", \"Mas Vnr Type\", \"Foundation\", \"Heating\", \"Central Air\", \"Garage Type\", \n",
        "                    \"Misc Feature\", \"Sale Type\", \"Sale Condition\"]\n",
        "    \n",
        "    # list the columns that need to be transformed\n",
        "    trans_col = []\n",
        "    for i in nominal_features:\n",
        "        if i in df.columns:\n",
        "            trans_col.append(i)\n",
        "            \n",
        "    # find out the unique values in each column\n",
        "    unique_counts = df[trans_col].apply(lambda x: len(x.value_counts())).sort_values()\n",
        "\n",
        "# set the threshold for the amount of unique values\n",
        "    nonunique_counts = unique_counts[unique_counts > threshval]\n",
        "\n",
        "    # drop the columns with unique values exceeding threshold value\n",
        "    df = df.drop(nonunique_counts.index, axis=1) \n",
        "    \n",
        "    # select the remaining text columns and convert it to categorical data\n",
        "    text_cols = df.select_dtypes(include=['object'])\n",
        "\n",
        "    for i in text_cols:\n",
        "        df[i] = df[i].astype('category')\n",
        "\n",
        "    # create dummy columns and drop the original columns\n",
        "    df = pd.concat([df,\n",
        "\n",
        " pd.get_dummies(df.select_dtypes(include=['category']))\n",
        "                        ], axis =1).drop(text_cols, axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "# test the function\n",
        "df = pd.read_csv('/kaggle/input/ameshousingdataset/AmesHousing.tsv', delimiter='\\t')\n",
        "transformed = transform_features(df)\n",
        "selected_features = select_features(transformed)\n",
        "test = train_and_test(selected_features)\n",
        "test\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "# The function accepts k parameter, k=0 (default) for holdout validation, k=1 for cross validation,\n",
        "# and k fold validation\n",
        "def train_and_test(df, k=0):\n",
        "    num_df = df.select_dtypes(include=['float', 'int'])\n",
        "    features = df.columns.drop('SalePrice')\n",
        "    lr = LinearRegression()\n",
        "\n",
        "    if k==0:\n",
        "        train = df[:1460]\n",
        "        test = df[1460:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlvUyZrjeo97"
      },
      "source": [
        "## 4. Train and Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYllZ0xBetTh"
      },
      "source": [
        "Now for the final part of the pipeline, training and testing. When iterating on different features, using simple validation is a good idea. Let's add a parameter named `k` that controls the type of cross validation that occurs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LmYg5JWeuGt"
      },
      "source": [
        "**Tasks**\n",
        "\n",
        "1. The optional `k` parameter should accept integer values, with a default value of `0`.\n",
        "\n",
        "2. When `k` equals `0`, perform holdout validation (what we already implemented):\n",
        "\n",
        "* Select the first `1460` rows and assign to `train`.\n",
        "* Select the remaining rows and assign to test.\n",
        "* Train on `train` and `test` on test.\n",
        "* Compute the `RMSE` and return.\n",
        "\n",
        "3. When k equals 1, perform simple cross validation:\n",
        "\n",
        "* Shuffle the ordering of the rows in the data frame.\n",
        "* Select the first 1460 rows and assign to `fold_one`.\n",
        "* Select the remaining rows and assign to `fold_two`.\n",
        "* Train on `fold_one` and test on `fold_two`.\n",
        "* Train on `fold_two` and test on `fold_one`.\n",
        "* Compute the average RMSE and return.\n",
        "\n",
        "4. When `k` is greater than `0`, implement k-fold cross validation using `k` folds:\n",
        "\n",
        "* Perform `k-fold` cross validation using k folds.\n",
        "* Calculate the average `RMSE` value and return this value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65zsZNJyfUnS"
      },
      "source": [
        "# train and test the model\n",
        "  lr.fit(train[features], train[\"SalePrice\"])\n",
        "        pred = lr.predict(test[features])\n",
        "        mse = mean_squared_error(test['SalePrice'], pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        \n",
        "    elif k==1:\n",
        "        # randomize order of rows\n",
        "        np.random.seed(1)\n",
        "        shuffled_index = np.random.permutation(df.index)\n",
        "        df = df.reindex(shuffled_index)\n",
        "        \n",
        "        train = df[:1460]\n",
        "        test = df[1460:]\n",
        "    \n",
        "    # train and test the model\n",
        "        lr.fit(train[features], train[\"SalePrice\"])\n",
        "        pred = lr.predict(test[features])\n",
        "        mse = mean_squared_error(test['SalePrice'], pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        \n",
        "    elif k==1:\n",
        "        # randomize order of rows\n",
        "        np.random.seed(1)\n",
        "        shuffled_index = np.random.permutation(df.index)\n",
        "        df = df.reindex(shuffled_index)\n",
        "        \n",
        "        train = df[:1460]\n",
        "        test = df[1460:]\n",
        "\n",
        "         train and test the model\n",
        "        lr.fit(train[features], train[\"SalePrice\"])\n",
        "        pred1 = lr.predict(test[features])\n",
        "        mse1 = mean_squared_error(test['SalePrice'], pred1)\n",
        "        rmse1 = np.sqrt(mse1)\n",
        "        \n",
        "        lr.fit(test[features], test[\"SalePrice\"])\n",
        "        pred2 = lr.predict(train[features])\n",
        "        mse2 = mean_squared_error(train['SalePrice'], pred2)\n",
        "        rmse2 = np.sqrt(mse2)\n",
        "        \n",
        "        rmse = (rmse1 + rmse2) / 2\n",
        "        \n",
        "        else:\n",
        "        kf = KFold(n_splits=k, shuffle=True, random_state=1)\n",
        "        mses = cross_val_score(estimator=lr, X=df[features], y=df['SalePrice'], scoring='neg_mean_squared_error', cv=kf)\n",
        "        rmse = np.mean(abs(mses)**0.5)\n",
        "\n",
        "    return rmse\n",
        "\n",
        "df = pd.read_csv('/kaggle/input/ameshousingdataset/AmesHousing.tsv', delimiter='\\t')\n",
        "transformed = transform_features(df)\n",
        "selected_features = select_features(transformed)\n",
        "test = train_and_test(selected_features, k=5)\n",
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "USb-p2gpRBHs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBRXPk5wflpP"
      },
      "source": [
        "## 5. Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JV5JSMQfp_o"
      },
      "source": [
        "That's it for the guided steps. Here's some potenial next steps that you can take:\n",
        "\n",
        "1. Continue iteration on feature engineering:\n",
        "* Research some other approaches to feature engineering online around housing data.\n",
        "* Visit the Kaggle kernels [page](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/kernels) page for this dataset to see approaches others took.\n",
        "\n",
        "2. Improve your feature selection:\n",
        "* Research ways of doing feature selection better with categorical columns (something we didn't cover in this particular course)."
      ]
    }
  ]
}